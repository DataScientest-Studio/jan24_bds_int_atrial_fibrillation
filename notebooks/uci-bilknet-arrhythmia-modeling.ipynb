{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7738334,"sourceType":"datasetVersion","datasetId":4522637}],"dockerImageVersionId":30664,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# First Modeling","metadata":{}},{"cell_type":"code","source":"## Import all the libraries\nimport pandas as pd\nimport numpy as np\n\n## Data Viz \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\n\n## Transformation\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n\n## for statistical tests\nfrom math import sqrt\nimport scipy\nfrom scipy.fft import fft, fftfreq\nimport statistics\nfrom statistics import mean\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n## SMOTE\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler,  ClusterCentroids\nfrom imblearn.metrics import classification_report_imbalanced, geometric_mean_score\nfrom sklearn.svm import SVC\n\n## Cluster Centroids\nfrom imblearn.under_sampling import ClusterCentroids\nfrom sklearn.cluster import KMeans\n\n## Modelling \nfrom sklearn import datasets, decomposition, ensemble, feature_selection, linear_model, metrics, model_selection, preprocessing, svm, tree\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.feature_selection import SelectFromModel, SelectKBest\nfrom sklearn.linear_model import LogisticRegressionCV, LogisticRegression\nfrom sklearn.metrics import roc_auc_score, accuracy_score, classification_report, confusion_matrix, explained_variance_score, f1_score, mean_absolute_error, mean_squared_error, precision_score, r2_score, recall_score, roc_curve, roc_auc_score, precision_recall_curve, average_precision_score\nfrom sklearn.model_selection import cross_validate, cross_val_predict, cross_val_score, GridSearchCV, ShuffleSplit, train_test_split\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import LinearSVC, SVC, SVR\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\n\nimport shap\n\nfrom sklearn.pipeline import Pipeline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom tqdm import tqdm\nimport time\n\nprint('Libraries imported successfully')","metadata":{"execution":{"iopub.status.busy":"2024-03-05T15:39:15.705521Z","iopub.execute_input":"2024-03-05T15:39:15.706543Z","iopub.status.idle":"2024-03-05T15:39:27.503069Z","shell.execute_reply.started":"2024-03-05T15:39:15.706503Z","shell.execute_reply":"2024-03-05T15:39:27.501925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Code for Kaggle\ndf = pd.read_csv('/kaggle/input/arrhythmia-preprocessed/arrhythmia_preprocessed_cleaned_classes_label(1).csv', sep=',', index_col=0)\nprint('Dataset imported successfully')","metadata":{"execution":{"iopub.status.busy":"2024-03-05T15:39:27.505263Z","iopub.execute_input":"2024-03-05T15:39:27.505841Z","iopub.status.idle":"2024-03-05T15:39:27.578988Z","shell.execute_reply.started":"2024-03-05T15:39:27.505810Z","shell.execute_reply":"2024-03-05T15:39:27.578058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Code for GitHub\n# df = pd.read_csv('arrhythmia_preprocessed_cleaned_classes_label.csv')","metadata":{"execution":{"iopub.status.busy":"2024-03-05T15:39:27.580280Z","iopub.execute_input":"2024-03-05T15:39:27.580686Z","iopub.status.idle":"2024-03-05T15:39:27.585295Z","shell.execute_reply.started":"2024-03-05T15:39:27.580654Z","shell.execute_reply":"2024-03-05T15:39:27.584120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Seperate Features, Standardize and Split","metadata":{}},{"cell_type":"code","source":"# Separate features and target variable\nX = df.drop(['class','label'], axis=1)  # Features\ny = df['label']  # Target variable\n\n# Standardize the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\n# Save X train and X_test without PCA \nX_train_saved = X_train\nX_test_saved = X_test\ny_train_saved = y_train\ny_test_saved = y_test","metadata":{"execution":{"iopub.status.busy":"2024-03-05T15:39:27.588095Z","iopub.execute_input":"2024-03-05T15:39:27.589044Z","iopub.status.idle":"2024-03-05T15:39:27.618025Z","shell.execute_reply.started":"2024-03-05T15:39:27.589008Z","shell.execute_reply":"2024-03-05T15:39:27.616791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PCA - Principal Component Analysis","metadata":{}},{"cell_type":"code","source":"# Define different numbers of components to try\nn_components_list = [30,40,50,55,60,65,70,75,100]\ncumulative_variance_ratios = []\nfor n_components in n_components_list:\n    \n    # Instantiate PCA with desired number of components\n    pca = PCA(n_components=n_components)\n    \n    # Fit PCA to the standardized data\n    pca.fit(X_scaled)\n    \n    # Transform the data into the new feature space\n    X_pca = pca.transform(X_scaled)\n    \n    # Save the results in individual dataframes\n    globals()[f'X_pca_{n_components}'] = pd.DataFrame(X_pca)\n    \n    # Get the explained variance ratio\n    explained_variance_ratio = pca.explained_variance_ratio_\n    cumulative_variance_ratio = sum(explained_variance_ratio)\n    cumulative_variance_ratios.append(cumulative_variance_ratio)\n    \n    # Print cumulative variance ratio\n    print(f'Cumulative variance ratio with {n_components} components:', cumulative_variance_ratio)\n\n# Plot cumulative variance ratio\nplt.figure(figsize=(8, 6))\nplt.plot(n_components_list, cumulative_variance_ratios, marker='o', linestyle='-')\nplt.title('Cumulative Variance Ratio vs. Number of Components', fontsize=14)\nplt.xlabel('Number of Components', fontsize=14)\nplt.ylabel('Cumulative Variance Ratio', fontsize=14)\nplt.xticks(n_components_list)\nplt.grid(True)\n\n# Add annotations\nfor i, txt in enumerate(cumulative_variance_ratios):\n    plt.annotate(f'{txt:.2f}', (n_components_list[i], cumulative_variance_ratios[i]), textcoords=\"offset points\", xytext=(0,10), ha='center')\nplt.tight_layout()\nplt.savefig('PCA_cumulative_variance_ratio_vs_nr_components.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-05T15:39:27.619514Z","iopub.execute_input":"2024-03-05T15:39:27.620036Z","iopub.status.idle":"2024-03-05T15:39:28.783028Z","shell.execute_reply.started":"2024-03-05T15:39:27.619986Z","shell.execute_reply":"2024-03-05T15:39:28.781908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Re-run PCA with the selected number of components \npca = PCA(n_components=.9)\nX_pca = pca.fit_transform(X_scaled)\n\n# Get the explained variance ratio\nexplained_variance_ratio = pca.explained_variance_ratio_\n\n# Visualize the explained variance ratio\nplt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.8)\nplt.xlabel('Principal Component')\nplt.ylabel('Explained Variance Ratio')\nplt.title('Explained Variance Ratio for Principal Components')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-05T15:39:28.784493Z","iopub.execute_input":"2024-03-05T15:39:28.784908Z","iopub.status.idle":"2024-03-05T15:39:29.268510Z","shell.execute_reply.started":"2024-03-05T15:39:28.784869Z","shell.execute_reply":"2024-03-05T15:39:29.267378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setting the PCA Threshold for the Training & Test Datasets","metadata":{}},{"cell_type":"code","source":"# PCA on training data set \npca = PCA(n_components = .2)\nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.transform(X_test)\n\nprint(pca.n_components_)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T15:39:29.269953Z","iopub.execute_input":"2024-03-05T15:39:29.270884Z","iopub.status.idle":"2024-03-05T15:39:29.310582Z","shell.execute_reply.started":"2024-03-05T15:39:29.270839Z","shell.execute_reply":"2024-03-05T15:39:29.309135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize PCA with 'mle' to automatically determine the number of components\n#pca = PCA(n_components='mle')\n# Fit PCA to the standardized data\n#pca.fit(X_scaled)\n# Get the number of components that explain at least 90% of the variance\n#n_components = pca.explained_variance_ratio_.cumsum().searchsorted(0.90) + 1\n# Print the selected number of components\n#print(f'Selected number of components: {n_components}')","metadata":{"execution":{"iopub.status.busy":"2024-03-05T15:39:29.312724Z","iopub.execute_input":"2024-03-05T15:39:29.313241Z","iopub.status.idle":"2024-03-05T15:39:29.318766Z","shell.execute_reply.started":"2024-03-05T15:39:29.313203Z","shell.execute_reply":"2024-03-05T15:39:29.317054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#SMOTE\n#smo = SMOTE()\n#X_train_sm, y_train_sm = smo.fit_resample(X_train, y_train)\n\n#print(\"Shape of X_train resampled with smote:\", X_train_sm.shape)\n#print(\"Shape of y_train resampled with smote:\", y_train_sm.shape)\n#print('SMOTE :', dict(pd.Series(y_train_sm).value_counts()))","metadata":{"execution":{"iopub.status.busy":"2024-03-05T15:39:29.320843Z","iopub.execute_input":"2024-03-05T15:39:29.321313Z","iopub.status.idle":"2024-03-05T15:39:29.331061Z","shell.execute_reply.started":"2024-03-05T15:39:29.321276Z","shell.execute_reply":"2024-03-05T15:39:29.329416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Set X_train and X_test ","metadata":{}},{"cell_type":"code","source":"X_train = X_train_pca\nX_test = X_test_pca\nprint(X_train.shape)\nprint(X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T15:39:29.336693Z","iopub.execute_input":"2024-03-05T15:39:29.337197Z","iopub.status.idle":"2024-03-05T15:39:29.348067Z","shell.execute_reply.started":"2024-03-05T15:39:29.337156Z","shell.execute_reply":"2024-03-05T15:39:29.345869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Testing","metadata":{}},{"cell_type":"markdown","source":"# Define Classifiers and parameters","metadata":{}},{"cell_type":"code","source":"# Define classifiers with specified parameters\nclf_lr = LogisticRegression(random_state=22, max_iter=2000)\nclf_rf = RandomForestClassifier(random_state=22)\nclf_svc = SVC(random_state=22)\nclf_en = LogisticRegression(penalty='elasticnet', solver='saga', max_iter=10000)\nclf_gb = GradientBoostingClassifier(random_state=42)\nclf_ada = AdaBoostClassifier()\nclf_xgb = xgb.XGBClassifier()\n\n\n# Define parameter grids for each classifier\nparam_grid_lr = [{'C': [c], 'penalty': [penalty]} for c in np.logspace(-4, 2, 9) for penalty in ['l1', 'l2']]\n\nparam_grid_rf = [{'n_estimators': [10, 50, 100, 250, 500, 1000], \n                  'min_samples_leaf': [1, 3, 5], \n                  'max_features': ['sqrt', 'log2']}]\n\nparam_grid_svc = {'C': np.logspace(-4, 2, 9), 'kernel': ['linear', 'rbf']}\nparam_grid_svc_list = [{'C': [c], 'kernel': [kernel]} for c in np.logspace(-4, 2, 9) for kernel in ['linear', 'rbf']]\n\nparam_grid_en = {'C': np.logspace(-4, 2, 9), 'l1_ratio': np.linspace(0.1, 0.9, 9)}\nparam_grid_en_list = [{'C': [C], 'l1_ratio': [l1_ratio]} for C in np.logspace(-4, 2, 9) for l1_ratio in np.linspace(0.1, 0.9, 9)]\n\n\nparam_grid_gb = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1.0], 'max_depth': [3, 5, 7]}\nparam_grid_gb_list = [{'n_estimators': [n_estimators], 'learning_rate': [learning_rate], 'max_depth': [max_depth]} \n                                 for n_estimators in [50, 100, 200] \n                                 for learning_rate in [0.01, 0.1, 1.0] \n                                 for max_depth in [3, 5, 7]]\n\nparam_grid_ada = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1.0]}\nparam_grid_ada_list = [{'n_estimators': [n_estimators], 'learning_rate': [learning_rate]} \n                       for n_estimators in [50, 100, 200] \n                       for learning_rate in [0.01, 0.1, 1.0]]\n\nparam_grid_xgb = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1.0], 'max_depth': [3, 5, 7]}\nparam_grid_xgb_list = [{'n_estimators': [n_estimators], 'learning_rate': [learning_rate], 'max_depth': [max_depth]} \n                       for n_estimators in [50, 100, 200] \n                       for learning_rate in [0.01, 0.1, 1.0] \n                       for max_depth in [3, 5, 7]]\n\nprint('parameters set')","metadata":{"execution":{"iopub.status.busy":"2024-03-05T15:39:29.350380Z","iopub.execute_input":"2024-03-05T15:39:29.351621Z","iopub.status.idle":"2024-03-05T15:39:29.382328Z","shell.execute_reply.started":"2024-03-05T15:39:29.351565Z","shell.execute_reply":"2024-03-05T15:39:29.380958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training Function with Progress Bar and Score Output","metadata":{}},{"cell_type":"code","source":"# Define train_model function\ndef train_model(clf, param_grid, X_train, y_train, X_test, y_test):\n    # Initialize GridSearchCV with the classifier and parameter grid\n    gcv = GridSearchCV(clf, param_grid, cv=3, refit=True)\n\n    # Calculate total number of iterations for progress bar\n    total_iterations = len(param_grid) * 3\n\n    # Train the model with tqdm progress bar\n    with tqdm(total=total_iterations, desc=\"Training Progress\") as pbar:\n        for params in param_grid:\n            for fold in range(3):\n                # Update progress bar\n                pbar.update(1)\n\n                # Set parameters individually\n                for param_name, param_value in params.items():\n                    setattr(clf, param_name, param_value)\n\n                # Train the model\n                gcv.fit(X_train, y_train)\n\n    # Make predictions\n    train_predictions = gcv.predict(X_train)\n    test_predictions = gcv.predict(X_test)\n\n    # Calculate evaluation metrics\n    train_acc = accuracy_score(y_train, train_predictions)\n    test_acc = accuracy_score(y_test, test_predictions)\n    train_precision = precision_score(y_train, train_predictions)\n    test_precision = precision_score(y_test, test_predictions)\n    train_recall = recall_score(y_train, train_predictions)\n    test_recall = recall_score(y_test, test_predictions)\n    train_f1 = f1_score(y_train, train_predictions)\n    test_f1 = f1_score(y_test, test_predictions)\n\n# Calculate AUROC scores\n    train_auroc = roc_auc_score(y_train, train_predictions)\n    test_auroc = roc_auc_score(y_test, test_predictions)     \n\n# Return model performance metrics\n    return {\n        'model': type(clf).__name__,\n        'best_params': gcv.best_params_,\n        'train_accuracy': train_acc,\n        'test_accuracy': test_acc,\n        'train_precision': train_precision,\n        'test_precision': test_precision,\n        'train_recall': train_recall,\n        'test_recall': test_recall,\n        'train_f1': train_f1,\n        'test_f1': test_f1,\n        'train_auroc': train_auroc,\n        'test_auroc': test_auroc,\n        'test_predictions': test_predictions\n    }\n\nprint('function defined')","metadata":{"execution":{"iopub.status.busy":"2024-03-05T16:31:57.369226Z","iopub.execute_input":"2024-03-05T16:31:57.369664Z","iopub.status.idle":"2024-03-05T16:31:57.380090Z","shell.execute_reply.started":"2024-03-05T16:31:57.369636Z","shell.execute_reply":"2024-03-05T16:31:57.379325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a function to print the results in a readable format\ndef print_results(results):\n    print(f\"Model: {results['model']}\")\n    print(f\"Best Parameters: {results['best_params']}\")\n    print(\"Training Metrics:\")\n    print(f\"  Accuracy: {results['train_accuracy']:.2f}\")\n    print(f\"  Precision: {results['train_precision']:.2f}\")\n    print(f\"  Recall: {results['train_recall']:.2f}\")\n    print(f\"  F1-score: {results['train_f1']:.2f}\")\n    print(f\"  Auroc-score: {results['train_auroc']:.2f}\")    \n    print(\"Test Metrics:\")\n    print(f\"  Accuracy: {results['test_accuracy']:.2f}\")\n    print(f\"  Precision: {results['test_precision']:.2f}\")\n    print(f\"  Recall: {results['test_recall']:.2f}\")\n    print(f\"  F1-score: {results['test_f1']:.2f}\")\n    print(f\"  Auroc-score: {results['test_auroc']:.2f}\")    \n    print()\n\nprint('function defined')\n\n# FYI for reffering back to scores later: \n# Access logistic regression recall score for testing\n#lr_test_recall = lr_results['test_recall']\n# Now you can use lr_test_recall for further analysis or comparison\n#print(\"Logistic Regression Recall Score for Testing:\", lr_test_recall)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T16:28:01.073268Z","iopub.execute_input":"2024-03-05T16:28:01.073746Z","iopub.status.idle":"2024-03-05T16:28:01.088490Z","shell.execute_reply.started":"2024-03-05T16:28:01.073708Z","shell.execute_reply":"2024-03-05T16:28:01.087419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"code","source":"# run the train model function for lr\nlr_results = train_model(clf_lr, param_grid_lr, X_train, y_train, X_test, y_test)\n# print the results\nprint_results(lr_results)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T16:32:00.191742Z","iopub.execute_input":"2024-03-05T16:32:00.192344Z","iopub.status.idle":"2024-03-05T16:32:08.691191Z","shell.execute_reply.started":"2024-03-05T16:32:00.192313Z","shell.execute_reply":"2024-03-05T16:32:08.690240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest","metadata":{}},{"cell_type":"code","source":"# run the train model function for rf\nrf_results = train_model(clf_rf, param_grid_rf, X_train, y_train, X_test, y_test)\n# print the results\nprint_results(rf_results)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T16:37:07.577281Z","iopub.execute_input":"2024-03-05T16:37:07.577658Z","iopub.status.idle":"2024-03-05T16:40:09.594634Z","shell.execute_reply.started":"2024-03-05T16:37:07.577629Z","shell.execute_reply":"2024-03-05T16:40:09.593596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVM","metadata":{}},{"cell_type":"code","source":"# run the train model function for svc\nsvc_results = train_model(clf_svc, param_grid_svc_list, X_train, y_train, X_test, y_test)\n# print the results\nprint_results(svc_results)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T16:40:09.596759Z","iopub.execute_input":"2024-03-05T16:40:09.597389Z","iopub.status.idle":"2024-03-05T16:41:17.488175Z","shell.execute_reply.started":"2024-03-05T16:40:09.597333Z","shell.execute_reply":"2024-03-05T16:41:17.487365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ElasticNet","metadata":{}},{"cell_type":"code","source":"# run the train model function for elasticnet\nen_results = train_model(clf_en, param_grid_en_list, X_train, y_train, X_test, y_test)\n# print the results\nprint_results(en_results)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T16:41:17.489354Z","iopub.execute_input":"2024-03-05T16:41:17.489983Z","iopub.status.idle":"2024-03-05T16:44:57.537406Z","shell.execute_reply.started":"2024-03-05T16:41:17.489950Z","shell.execute_reply":"2024-03-05T16:44:57.536341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Gradientboost","metadata":{}},{"cell_type":"code","source":"# run the train model function for gradientboost\ngb_results = train_model(clf_gb, param_grid_gb_list, X_train, y_train, X_test, y_test)\n# print the results\nprint_results(gb_results)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T16:44:57.539663Z","iopub.execute_input":"2024-03-05T16:44:57.539985Z","iopub.status.idle":"2024-03-05T17:02:54.140934Z","shell.execute_reply.started":"2024-03-05T16:44:57.539958Z","shell.execute_reply":"2024-03-05T17:02:54.139778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Adaboost","metadata":{}},{"cell_type":"code","source":"# run the train model function for adaboost\nada_results = train_model(clf_ada, param_grid_ada_list, X_train, y_train, X_test, y_test)\n# print the results\nprint_results(ada_results)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:02:54.142373Z","iopub.execute_input":"2024-03-05T17:02:54.142683Z","iopub.status.idle":"2024-03-05T17:05:33.324840Z","shell.execute_reply.started":"2024-03-05T17:02:54.142657Z","shell.execute_reply":"2024-03-05T17:05:33.323795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"# run the train model function for xgboost\nxgb_results = train_model(clf_xgb, param_grid_xgb_list, X_train, y_train, X_test, y_test)\n# print the results\nprint_results(xgb_results)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:05:33.326072Z","iopub.execute_input":"2024-03-05T17:05:33.326391Z","iopub.status.idle":"2024-03-05T17:11:51.720607Z","shell.execute_reply.started":"2024-03-05T17:05:33.326351Z","shell.execute_reply":"2024-03-05T17:11:51.719515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Comparison","metadata":{}},{"cell_type":"code","source":"# Define model names and their corresponding results\nmodels = ['Log Regression', 'Random Forest', 'SVM', 'ElasticNet', 'AdaBoost', 'GradientBoost', 'XGBoost']\nresults = [lr_results, rf_results, svc_results, en_results, ada_results, gb_results, xgb_results]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract test and train accuracy scores for each model\ntest_accuracies = [result['test_accuracy'] for result in results]\ntrain_accuracies = [result['train_accuracy'] for result in results]\n\n# Extract test and train recall scores for each model\ntest_recalls = [result['test_recall'] for result in results]\ntrain_recalls = [result['train_recall'] for result in results]\n\n# Bar width\nbar_width = 0.2\nindex = np.arange(len(models))\n\n# Plotting\nplt.figure(figsize=(12, 8))\n\n# Plot test accuracy\nplt.bar(index, test_accuracies, bar_width, color='lightblue', edgecolor='black', hatch='/', label='Test Accuracy')\n# Plot train accuracy\nplt.bar(index + bar_width, train_accuracies, bar_width, color='lightgreen', edgecolor='black', hatch='\\\\', label='Train Accuracy')\n\n# Plot test recall\nplt.bar(index + 2*bar_width, test_recalls, bar_width, color='lightcoral', edgecolor='black', hatch='x', label='Test Recall')\n# Plot train recall\nplt.bar(index + 3*bar_width, train_recalls, bar_width, color='lightyellow', edgecolor='black', hatch='.', label='Train Recall')\n\nplt.xlabel('Model')\nplt.ylabel('Scores')\nplt.title('Comparison of Model Performances')\nplt.xticks(index + 1.5*bar_width, models)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-05T16:20:01.963273Z","iopub.execute_input":"2024-03-05T16:20:01.963973Z","iopub.status.idle":"2024-03-05T16:20:02.382763Z","shell.execute_reply.started":"2024-03-05T16:20:01.963941Z","shell.execute_reply":"2024-03-05T16:20:02.381731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# AUROC for best performing models","metadata":{}},{"cell_type":"code","source":"# Define model names and their corresponding results\nmodels = ['Log Regression', 'Random Forest', 'SVM', 'ElasticNet', 'AdaBoost', 'GradientBoost', 'XGBoost']\nresults = [lr_results, rf_results, svc_results, en_results, ada_results, gb_results, xgb_results]","metadata":{"execution":{"iopub.status.busy":"2024-03-05T16:17:39.990951Z","iopub.execute_input":"2024-03-05T16:17:39.991314Z","iopub.status.idle":"2024-03-05T16:17:39.996878Z","shell.execute_reply.started":"2024-03-05T16:17:39.991287Z","shell.execute_reply":"2024-03-05T16:17:39.995906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define model names and their corresponding results\nmodels = ['Log Regression', 'Random Forest', 'SVM', 'ElasticNet', 'AdaBoost', 'GradientBoost', 'XGBoost']\nresults = [lr_results, rf_results, svc_results, en_results, ada_results, gb_results, xgb_results]\n\n\n# Plotting ROC curves for each model\nplt.figure(figsize=(10, 8))\n\nfor model_name, result in zip(models, results):\n    # Compute ROC curve for test data\n    fpr, tpr, _ = roc_curve(y_test, result['test_predictions'])\n    \n    # Plot ROC curve\n    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {result[\"test_auroc\"]:.2f})')\n\n# Plot ROC curve for random guessing (baseline)\nplt.plot([0, 1], [0, 1], linestyle='--', color='black', label='Random Guessing')\n\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:14:35.167321Z","iopub.execute_input":"2024-03-05T17:14:35.167706Z","iopub.status.idle":"2024-03-05T17:14:35.573478Z","shell.execute_reply.started":"2024-03-05T17:14:35.167678Z","shell.execute_reply":"2024-03-05T17:14:35.572328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###","metadata":{"execution":{"iopub.status.busy":"2024-03-05T16:15:01.271002Z","iopub.status.idle":"2024-03-05T16:15:01.271899Z","shell.execute_reply.started":"2024-03-05T16:15:01.271596Z","shell.execute_reply":"2024-03-05T16:15:01.271624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize variables to keep track of the highest recall value and the corresponding model\nmax_recall = 0.0\nbest_model = None\n\n# Iterate through each model's result\nfor result in results:\n    model_name = result['model']\n    test_recall = result['test_recall']\n    \n    # Check if the current model has higher recall than the previous maximum\n    if test_recall > max_recall:\n        max_recall = test_recall\n        best_model = model_name\n\nprint(f\"The model with the highest recall is {best_model} with a recall of {max_recall:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:20:37.102075Z","iopub.execute_input":"2024-03-05T17:20:37.102444Z","iopub.status.idle":"2024-03-05T17:20:37.109414Z","shell.execute_reply.started":"2024-03-05T17:20:37.102415Z","shell.execute_reply":"2024-03-05T17:20:37.108260Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##","metadata":{"execution":{"iopub.status.busy":"2024-03-05T16:15:01.279506Z","iopub.status.idle":"2024-03-05T16:15:01.281189Z","shell.execute_reply.started":"2024-03-05T16:15:01.280738Z","shell.execute_reply":"2024-03-05T16:15:01.280777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components=.99)\n    \n# Define the pipeline\npipeline = Pipeline([\n    ('pca', PCA()),\n    ('log_reg', LogisticRegression())\n])\n\n# Define parameter grid for PCA and logistic regression\nparam_grid = {\n    'pca__n_components': range(1, min(X_train.shape[1], 200)),  # Adjust range as needed\n    # Add any other hyperparameters for Logistic Regression if needed\n    # 'log_reg__C': [0.1, 1, 10],  # Example hyperparameter for Logistic Regression\n}\n\n# Perform grid search with cross-validation\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='recall')\ngrid_search.fit(X_train, y_train)\n\n# Get the best parameters and best score\nbest_params = grid_search.best_params_\nbest_score = grid_search.best_score_\n\nprint(\"Best parameters:\", best_params)\nprint(\"Best cross-validation accuracy:\", best_score)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:22:02.348352Z","iopub.execute_input":"2024-03-05T17:22:02.348744Z","iopub.status.idle":"2024-03-05T17:22:02.438461Z","shell.execute_reply.started":"2024-03-05T17:22:02.348714Z","shell.execute_reply":"2024-03-05T17:22:02.437386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\n\n# Perform grid search with cross-validation\ngrid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\ngrid_search.fit(X_train, y_train)\n\n# Get the results\nresults = grid_search.cv_results_\nn_components = param_grid['pca__n_components']\nmean_scores = results['mean_test_score']\nstd_scores = results['std_test_score']\n\n# Plot results\nplt.errorbar(n_components, mean_scores, yerr=std_scores, fmt='-o')\nplt.xlabel('Number of Principal Components')\nplt.ylabel('Cross-validation Accuracy')\nplt.title('Cross-validation Accuracy vs. Number of Principal Components')\nplt.xticks(range(0, len(n_components), 10), rotation=90)\nplt.subplots_adjust(wspace=0.5)\nplt.grid(True)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:22:05.632448Z","iopub.execute_input":"2024-03-05T17:22:05.632835Z","iopub.status.idle":"2024-03-05T17:22:05.931390Z","shell.execute_reply.started":"2024-03-05T17:22:05.632805Z","shell.execute_reply":"2024-03-05T17:22:05.930304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract the final model (Logistic Regression) from the pipeline\nlog_reg_model = pipeline.named_steps['log_reg']\n\n# Ensure that the logistic regression model is fitted with training data\nlog_reg_model.fit(X_train, y_train)\n\n# Initialize a SHAP explainer with the logistic regression model\nexplainer = shap.Explainer(log_reg_model, X_train)\n\n# Calculate SHAP values for the testing set\nshap_values = explainer(X_test)\n\n# Plot the SHAP summary plot\nshap.summary_plot(shap_values, X_test, feature_names=X.columns.tolist(), class_names=y.unique())","metadata":{"execution":{"iopub.status.busy":"2024-03-05T17:24:54.239125Z","iopub.execute_input":"2024-03-05T17:24:54.239515Z","iopub.status.idle":"2024-03-05T17:24:54.610380Z","shell.execute_reply.started":"2024-03-05T17:24:54.239484Z","shell.execute_reply":"2024-03-05T17:24:54.609066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}